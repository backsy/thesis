# Database generation {#dbs}

## Inital data

Eli Ltd collects the binary flight logs that are saved by arducopter firmware aboard the multirotor platform. The binary files contain _MAVLink_ protocol packets. _MAVLink_ stands for _Micro Air Vehicle Message Marshalling Library_. 

> _MAVLink is a very lightweight, header-only message library for communication between drones and/or ground control stations. It consists primarily of message-set specifications for different systems ("dialects") defined in XML files, and Python tools that convert these into appropriate source code for supported languages._^[@mavlink]

Not all possible messages that the autopilot generates are sent to the GCS. It is configurable which packets are saved, which are sent to the GCS and which are dropped. In general the GCS receives some subset of all messages that is important for operating the multirotor. In the logs a bigger subset is recorded. For the logs present most important messages and communications are saved but extremely high resolution data <!-- should find the frequency --> is not saved. As such over a thousand logs have been collected. The total number of binary logs available is 1363 and amount to 57.2 GB of data. This would average 41966 KB per log. Among them are logs that are not actual flights but rather tests on the bench. These logs inflate the number of logs available. Some logs do contain the highest amounts of data and can be several hundred MB large and thus inflate the average. A normal flight can be expected to be between 4 to 100 MB's. An average 40+ minute flight is expected to be over 40 MB.


## Choosing the output data type

In this thesis _R_^[@r] programming language is used for data analysis. _R_ is a language and environment for statistical computing and graphics. As it is open source no licences are required to set it up and use. This brings statistical computing and data science to the hands of everyone interested in the topic and avoids expensive mathematics suites.

The binary data in the log files is not readily accessible from _R_. As a result it is necessary to convert the data to some other format. The _pymavlink_ project - a *python* implementation of MAVLink protocol^[@pymav] - has a few tools that do just that. First of the options is to convert the log into a CSV^[Comma separated values file is a text file where a comma is used to separate values.] file. _Mavlogdump.py_^[@pydump] is the tool available for conversion. With CSV files comes a caveat - you may only export one message type at a time. This is to be expected since the the CSV files separate table columns with commas and the log files have message types with different lengths of columns. Example lines of CSV file taken from a flight log containing NTUN packets.

```
timestamp,TimeUS,DPosX,DPosY,PosX,PosY,DVelX,DVelY,VelX,VelY,DAccX,...
1521817093.27547407,688646373,-489.633148193,86.0853424072...
1521817093.37760210,688748501,-489.633148193,86.0853424072...
1521817093.47829199,688849191,-489.633148193,86.0853424072...
```

In _R_ it is very simple to import CSV files and this at first seemed a viable candidate file type for storage. However since there are many different types of messages, each log would in turn be converted to as many CSV files. Potentially leaving us with tens of thousands of files. This means _R_ would have to import the relevant files one by one and processing them. With thousands of files to import the analysis time would be negatively impacted as the data import poses an overhead.

Another option is to use JSON^[@json]. JSON is a data-interchange format designed to be easily read and written by humans as well as to be easily parsed and generated by machines. There are two structures in JSON:

* A collection of name/value pairs.

* An ordered list of values.

Both are familiar to programmers using the _C_-family languages. Both can be seen in the following example:

```{r eval=FALSE}
{"menu": {
  "id": "file",
  "value": "File",
  "popup": {
    "menuitem": [
      {"value": "New", "onclick": "CreateNewDoc()"},
      {"value": "Open", "onclick": "OpenDoc()"},
      {"value": "Close", "onclick": "CloseDoc()"}
    ]
  }
}}
```

Two consecutive entries from the binary log converted to json format using _mavlogdump.py_ look like this:

```{r eval=FALSE}
{"meta": {
    "timestamp": 1521817579.039407, 
    "type": "PIDP"
  }, 
  "data": {
    "D": -0.0005177092389203608, 
    "TimeUS": 1174410306, 
    "I": 0.0324220173060894, 
    "AFF": 0.0, 
    "Des": -0.012198338285088539, 
    "P": -0.005622388795018196, 
    "FF": -0.0
  }
}
{"meta": {
    "timestamp": 1521817579.039419, 
    "type": "PIDY"
  }, 
  "data": {
    "D": -0.0, 
    "TimeUS": 1174410318, 
    "I": -0.08129391074180603, 
    "AFF": 0.0, 
    "Des": 0.006200382951647043, 
    "P": 0.008107485249638557, 
    "FF": 0.0
  }
}
```

_R_ has libraries to deal with JSON data. Such as _jsonlite_ ^[@jlite] and _rjson_^[@rjon]. As with CSV, JSON files need to be imported one by one, analyzed and unloaded. This has considerable overhead. Further more an output file from a 47.6 MB log file becomes 649.8 MB. That is an increase of size of thirteen times! The library of logs that is 57.2 GB becomes 743.6 GB of JSON data as a result. At least there are an order of magnitude lower amount of files.

The final option to consider is _SQLite_^[@sqlite]. _SQLite_ is a _SQL_ database engine that is self-contained and serverless. It is designed to be _SQL_ database, but without concerning itself with a server process and user management and other common _SQL_ attributes. As such its designed to be a competitor to both JSON and CSV. The benefit of _SQLite_ is that it can be used by various third party programs and programming languages. _Python_ and _Tcl_ have _SQLite_ built in. Raw data can be imported from CSV files and the data can be compressed to similar sizes to _Zip_ files. Since the database is a single file it can easily be written to a USB memory stick or with smaller databases emailed to a colleague directly. Given some understanding of _SQL_ _SQLite_ offers an easy to use database.

_R_ language has adapters to connect to _SQL_ databases such as _RODBC_^[@1], _RJDBC_^[@3], _bigrquery_^[@4] and many others among which is _RSQLite_^[@5]. _RSQLite_ embeds the _SQLite_ database engine in _R_, providing a _DBI_^[@6]-compliant interface. _DBI_ is an _R_ package that defines a common interface between _R_ and database management systems (_DBMS_). With _RSQLite_ package it is possible to directly manipulate data in _SQLite_ database.


## Tidy data

Tidy data is a set of principles that help organize data in data sets. This helps make data cleaning easier and faster by not having to start from scratch every time. Another benefit of tidy data is that it is easier to design data analysis tools which can assume that the data input is always tidy. Both benefits help the data analyser to focus on the underlying problem rather than managing data half the time^[@tidy-data]. 

-----------------------------------------------------------------------------
        &nbsp;           mpg    cyl   disp   hp    drat    wt     qsec    vs 
----------------------- ------ ----- ------ ----- ------ ------- ------- ----
     **Mazda RX4**        21     6    160    110   3.9    2.62    16.46   0  

   **Mazda RX4 Wag**      21     6    160    110   3.9    2.875   17.02   0  

    **Datsun 710**       22.8    4    108    93    3.85   2.32    18.61   1  

  **Hornet 4 Drive**     21.4    6    258    110   3.08   3.215   19.44   1  

 **Hornet Sportabout**   18.7    8    360    175   3.15   3.44    17.02   0  
-----------------------------------------------------------------------------

Table: Tidy data.

 
------------------------------------------
        &nbsp;           am   gear   carb 
----------------------- ---- ------ ------
     **Mazda RX4**       1     4      4   

   **Mazda RX4 Wag**     1     4      4   

    **Datsun 710**       1     4      1   

  **Hornet 4 Drive**     0     3      1   

 **Hornet Sportabout**   0     3      2   
------------------------------------------

Here we are looking at the _mtcars_ data set that is inbuilt in _R_. In tidy data:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

This data set is already tidy. Each row is an *observation* and each column represents a *variable*. Every element in the table is a *value*. Any other arrangement of data is considered *messy*. By having tidy data it is easy to manipulate data such as group by column info and tie break on another column. Tidy data is particularly well suited for vectored programming languages like _R_ where each observation of each variable is always paired.

The five most common problems with messy data sets are:

1. Column headers are values, not variable names.
2. Multiple variables are stored in one column.
3. Variables are stored in both rows and columns.
4. Multiple types of observational units are stored in the same table.
5. A single observational unit is stored in multiple tables.

-----------------------------------------------------------------------
religion      `<$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k`
------------  ------- --------- --------- --------- --------- ---------
Agnostic      27        34        60        81        76       137

Atheist       12        27        37        52        35        70

Buddhist      27        21        30        34        33        58

Catholic      418       617       732       670       638      1116

Don`t know    15        14        15        11        10        35

Evangelical   575       869      1064       982       881      1486

Hindu         1         9         7         9        11        34

Historically  228       244       236       238       197       223

Jehovah's     20        27        24        24        21        30

Jewish        19        19        25        25        30        95
-----------------------------------------------------------------------

Table: Column headers as variables.

Here we can see a table of data where the column header - the variable itself is a value. This can help make very dense and informative tables but for working with the data it is not tidy. If we wish to separate the data into segments of 5000 dollars then we can say that this table also has multiple variables in a single column. Rest of the problems we will not look at.

## Database normalization

_ArduPilot_^[@ardu] development team hosts an organization of projects on _GitHub_^[An online platform for version controlling software development] that contains several useful projects such as the _ardupilot_ autopilot firmware codebase for multirotors, planes, rovers and much more. Among the project is _pymavlink_ project which provides a _python_ _MAVLink_ interface and utilities. Among those utilities are tools mentioned in chapter [Choosing the output data type]. With these tools we determined that none of them fit our needs exactly but those tools can be used as a source of information for building our own tools.

The data storage format was decided to be _SQLite_ database. To increase the efficiency of working with the data the data set should be tidy. By taking that into account when designing the _SQLite_ database, time can be saved by not having to separately tidy up the data set before analysis. Further more Edgar F. Codd^[@codd] defined _normal_ _forms_ to permit querying and manipulation by a universal data language^[One such language is _SQL_]. The third normal form (3NF)^[@3fn] is considered as being tidy data. Simply by designing the database using the normal forms, especially the third normal form allows us to reach tidy data.

By instructing _mavlogdump.py_ to dump the logs from binary form to a readable text file we can see the general shape of the data in the binary logs.

`mavlogdump.py log.BIN > dump.txt`

|Time                   | Message |  Data        |
|:----------------------|:--------|:-------------|
|2018-03-23 17:26:57.12:| ATT     |  Inner Table |
|2018-03-23 17:26:57.12:| RATE    |  Inner Table |
|2018-03-23 17:26:57.12:| PIDR    |  Inner Table |

: (\#tab:dump) Excerpt from _dump.txt_.


Message|Data
:------|------:
TimeUS|2412496394
DesRoll|-2.41
Roll|-2.26
DesPitch|2.64
Pitch|2.8
DesYaw|96.07
Yaw|96.28
ErrRP|0.01
ErrYaw|0.02

: (\#tab:att) Message ATT.


Message|Data
:------|------:
TimeUS|2412496408
RDes|-0.977470874786
R|0.30203345418
ROut|-0.0680121853948
PDes|-1.36304438114
P|-1.30311119556
POut|0.0843362286687
YDes|-1.62430250645
Y|-0.70737850666
YOut|-0.1534512043
ADes|1.88155674934
A|-2.99711227417
AOut|0.368027120829

: (\#tab:rate) Message RATE.


Message|Data
:------|------:
TimeUS|2412496431
Des|-0.0153276510537
P|-0.00370784359984
I|-0.0374843552709
D|-0.0268199834973
FF|-0.0
AFF|0.0

Table: (\#tab:pidr) Message PIDR.


First normal form is a property of a relation where each attribute of the domain contains only indivisible values and the value of each attribute contains only a single value from that domain^[@1fn]. In table \@ref(tab:dump) we see that there are inner tables in the message. This is in violation of the requirement of atomic values. The inner tables need to be removed from the message and separated into another table. 

```{r 1fn, fig.cap="Message and ATT relationship.", echo=FALSE}
include_graphics(path = "./figure/1fn.PNG")
```

Figure \@ref(fig:1fn) has two tables where _Message_ table represents all the different messages and table _ATT_ represents any single _data_ table of a message - here the _ATT_ message. From tables \@ref(tab:att), \@ref(tab:rate) and \@ref(tab:pidr) we can see that the inner tables are of different shape. This means that each message requires a separate table. The current _Message_ structure does not permit more than one _data_ table to be used.

Another option would be to unwrap the inner _data_ table to be a part of the outer _Message_ table. Here we run into a similar problem. Since the inner _data_ tables are of different shape, the single _Message_ table would have to contain each possible variable in the logs and be empty most of the time. This table is realizable in a database.

Id|Time|Message|TimeUS|DesRoll|Roll|..|RDes|R|..|Des|P
---|----------------|:-----|------|------|-----|-|----|----|-|---|----
123|time|ATT|value|value|value|..|NA|NA|..|NA|NA
124|time|RATE|value|NA|NA|..|value|value|..|NA|NA
125|time|PIDR|value|NA|NA|..|NA|NA|..|value|value

: (\#tab:message) _Message_ table.

This table contains mostly _NA_ values as for every message only a subset of variables is relevant. When working with this table, the _NA_ values need to be removed. We can solve this problem by converting this table from wide form to long form. Since we are storing several flights into the same database we need to add a flight identificator.

Id|Flight|Message|Timestamp|Parameter|Value
--|:---:|:---|-----------------|:---|----------:|
125|4|ATT|2018-03-23 17:26:57.12|TimeUS|2412496394
126|4|ATT|2018-03-23 17:26:57.12|DesRoll|-2.41
127|4|ATT|2018-03-23 17:26:57.12|Roll|-2.26
...|...|...|...|...|...
134|4|RATE|2018-03-23 17:26:57.12|TimeUS|2412496408
135|4|RATE|2018-03-23 17:26:57.12|RDes|-0.977470874786
136|4|RATE|2018-03-23 17:26:57.12|R|0.30203345418
...|...|...|...|...|...
145|4|PIDR|2018-03-23 17:26:57.12|TimeUS|2412496431
146|4|PIDR|2018-03-23 17:26:57.12|Des|-0.0153276510537
147|4|PIDR|2018-03-23 17:26:57.12|P|-0.00370784359984

: (\#tab:longtable) Long table.

In table \@ref(tab:longtable) we added flight identificator but we would like to have more information on the flights. To reduce metadata repetition another table is needed. There is also quite a lot of repetition in other values in the table. The message type and timestamp repeat for each parameter in the inner table. The parameters also repeat each time the message is repeated. Each of the repeating elements can be moved to a dedicated table.

```{r database, fig.cap="Final database schema.", echo=FALSE}
include_graphics(path = "./figure/databaseSchema.PNG")
```

## The code

The code starts off with taking as an input the folder in which the binary files is stored in. The folder is searched recursively so that sub folders containing log files are also searched through. The files are counted and the sizes saved. A filtering is done by file size. Files larger than 100 MB and smaller than 4 MB are skipped. After the first log file is processed the time taken is used to calculate an estimation as to how long the whole process will take. Further more the total time elapsed is also displayed.

_Mavutil.py_ from _pymavlink_ project is used for translating the binary data to in memory representation. From there the data is buffered and written to the _SQLite_ database. Each new message type adds its parameter types to _Parameter_ table as seen in figure \@ref(fig:database) and each new message type is added to the messages table. This allows for the specification for messages to change and several versions of the specification to be used in different logs. The message name and the parameter names are decoupled from each other. An assumption that the message specification does not change in a single log is made. 

After processing a log a new entry to the _Flight_ table is made. The metadata that is added is the path where the file was stored. As the logs are separated into folders by which multirotor it was obtained from or from which period in time or which version of the multirotor was used then the path gives some sort of information that could be used for filtering in the analysis stage.

Timestamps are allowed to repeat as searching through the whole table means more processing needed while creating the database. This leaves a possibility to create a secondary script that would look through the _Timestamp_ table and removing duplicates and substituting the _Id_ in the _Value_ table.

There is an overhead to writing into the _SQLite_ database. A single write entails opening the database connection, writing to the database and closing the connection. The opening and closing operation adds an overhead that over hundreds of thousands of writes becomes significant. To remedy this a bulk insert operation is available. Every 10 000 lines of the binary log a bulk write is done. That number is experimental in nature as in testing making the number bigger did not seem to save any processing time. Making it smaller however slows the processing down. There is a possibility that further optimization can be done.

In case where the script crashes or some error happens or new logs are to be added to the database the code first checks the existence of the database. If the database exists the supporting tables are read into memory to be tested against so that new messages and parameters get added to the database.

While _SQLite_ is a good option for an on disk storage of the database, other databases are available and have some useful features. One of which is the ability to use window functions on the database. This means that it is possible to use filter kernels on the data while still in database. Such functions using _SQLite_ require loading the relevant segment of data (such as a single flight) into _R_. An attempt was made to port the working _python_ code to instead of _SQLite_ to use _PostgreSQL_^[@postgresql]. The code does work but is orders of magnitude slower. Heavy optimization is needed to streamline the process and as _SQLite_ interface is simpler in nature this idea was left as something to be done later given more time. Moving to a full featured relational database management system would allow easier porting of the analysis to a web based service later, but that is not relevant in the context of this thesis.

The code for creating the database is written in _python_ and is added to the appendix. 












